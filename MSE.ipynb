{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pytorch_lightning import callbacks\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "from models import *\n",
    "import torch.nn  as nn\n",
    "from experiments.vae_experiment import VAEXperiment\n",
    "from experiments.vae_pix2pix_exp import Pix2pixExperiment\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TestTubeLogger\n",
    "from torch.utils.data import DataLoader \n",
    "from terrain_loader import TerrainDataset\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "with open(\"configs/vae_pix2pix.yml\", 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(\".\\n\\n\",exc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dataset = TerrainDataset(root = config['exp_params']['data_path'],\n",
    "                        train=False,\n",
    "                        hide_green=config['exp_params']['hide_green'],\n",
    "                        norm=config['exp_params']['norm'])\n",
    "\n",
    "sample_dataloader = DataLoader(dataset,\n",
    "                        batch_size= 1,\n",
    "                        num_workers=config['exp_params']['n_workers'],\n",
    "                        shuffle = True,\n",
    "                        drop_last=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    " \n",
    "! rsync -aP ada:/share3/shanthika_naik/pytorch_terrain_authoring/TerrainAuthoring_Pytorch/logs/VanillaVAE/version_0  /scratch/shan/\n",
    "! rsync -aP ada:/share3/shanthika_naik/pytorch_terrain_authoring/TerrainAuthoring_Pytorch/logs/VAE_PIX2PIX/log0/version_3 /scratch/shan"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "receiving incremental file list\n",
      "receiving incremental file list\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#Vae Model\n",
    "vae_model = vae_models[config['vae_model_params']['name']](**config['vae_model_params'])\n",
    "\n",
    "# pix2pix model\n",
    "gen_model = pix2pix_model[config['pix2pix_model_params']['gen_name']](config['exp_params']['in_channels'],config['exp_params']['out_channels'])\n",
    "disc_model = pix2pix_model[config['pix2pix_model_params']['disc_name']](config['exp_params']['in_channels'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "if config['vae_model_params']['load_model'] :\n",
    "    experiment_p2p = Pix2pixExperiment.load_from_checkpoint(config['pix2pix_model_params']['pretrained_model'], gen_model=gen_model,disc_model=disc_model,vae_model=vae_model,params=config['exp_params'])\n",
    "    experiment_vae = VAEXperiment.load_from_checkpoint(config['vae_model_params']['pretrained_model'], vae_model=vae_model,params=config['exp_params'])\n",
    "    print(\"[INFO] Loaded pretrained model\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] Loaded pretrained model\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "vae_model.eval()\n",
    "gen_model.eval()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (down1): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.3, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down2): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.3, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down3): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.3, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down4): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.3, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down5): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.3, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down6): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.3, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down7): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.3, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down8): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.3, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (up1): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (up2): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (up3): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (up4): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (up5): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (up6): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (up7): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (final): Sequential(\n",
       "    (0): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def denormalize(result):\n",
    "        # minv, maxv = torch.min(result), torch.max(result)\n",
    "        new = (result+1)*127.5\n",
    "        return torch.squeeze(new).detach().numpy().transpose((1,2,0)).astype(np.uint8)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def get_mse(ip,op ):\n",
    "\n",
    "    res = vae_model(ip)[0] \n",
    "     \n",
    "    res = gen_model(res) \n",
    "    res = denormalize(res)\n",
    "    op = denormalize(op)\n",
    "    # ip = denormalize(ip)\n",
    "    ip = (ip*255)\n",
    "    ip = torch.squeeze(ip).detach().numpy().transpose((1,2,0)).astype(np.uint8)\n",
    "    res = cv2.GaussianBlur(res, (5, 5), 0)\n",
    "\n",
    "    res_3 = cv2.GaussianBlur(res, (3,3), 0)\n",
    "    res_5 = cv2.GaussianBlur(res, (5,5), 0)\n",
    "    res_7 = cv2.GaussianBlur(res, (7,7), 0)\n",
    "    res_11 = cv2.GaussianBlur(res, (11,11), 0)\n",
    "    res_21 = cv2.GaussianBlur(res, (21,21), 0)\n",
    "\n",
    "\n",
    "    mse = np.mean((res-op)**2)\n",
    "    mse_3 = np.mean((res_3-op)**2)\n",
    "    mse_5 = np.mean((res_5-op)**2)\n",
    "    mse_7 = np.mean((res_7-op)**2)\n",
    "    mse_11 = np.mean((res_11-op)**2)\n",
    "    mse_21 = np.mean((res_21-op)**2)\n",
    "\n",
    "\n",
    "    return mse,mse_3,mse_5,mse_7,mse_11,mse_21"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def calculate_mse( ):\n",
    "    count=0\n",
    "    mse_loss = 0 \n",
    "    mse_loss_3 = 0 \n",
    "    mse_loss_5 = 0 \n",
    "    mse_loss_7 = 0 \n",
    "    mse_loss_11 = 0 \n",
    "    mse_loss_21 = 0 \n",
    "\n",
    "    for ip, op in sample_dataloader:\n",
    "        count+=1\n",
    "        ml,ml_3,ml_5,ml_7,ml_11,ml_21 = get_mse(ip,op )  \n",
    "        mse_loss += ml\n",
    "        mse_loss_3 +=ml_3\n",
    "        mse_loss_5 +=ml_5\n",
    "        mse_loss_7 +=ml_7\n",
    "        mse_loss_11 +=ml_11\n",
    "        mse_loss_21 +=ml_21\n",
    "\n",
    "    \n",
    "    return mse_loss,mse_loss_3,mse_loss_5,mse_loss_7,mse_loss_11,mse_loss_21"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "mse_loss,mse_loss_3,mse_loss_5,mse_loss_7,mse_loss_11,mse_loss_21 = calculate_mse()\n",
    "print(mse_loss,mse_loss_3,mse_loss_5,mse_loss_7,mse_loss_11,mse_loss_21)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mse_loss_1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "28.895053582564444"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('py36': conda)"
  },
  "interpreter": {
   "hash": "fcc4c848c274c8c61ae3b27da7a11b5844fd0739b66d43e7f4839429996fc46a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}